{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process on folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 1/4 [00:40<02:00, 40.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for multiple_pople1.jpg: 40.33 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 2/4 [01:25<01:26, 43.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for multiple_pople3.jpg: 45.53 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  75%|███████▌  | 3/4 [02:31<00:53, 53.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for mutiple_pople2.jpg: 65.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 4/4 [03:28<00:00, 52.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time for prs.jpg: 56.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from retinaface import RetinaFace\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def detect_image(input_folder, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Initialize tqdm to show progress bar\n",
    "    image_files = [f for f in os.listdir(input_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    progress_bar = tqdm(total=len(image_files), desc=\"Processing\")\n",
    "\n",
    "    # Iterate over the images in the input folder\n",
    "    for image_file in image_files:\n",
    "        start_time = time.time()\n",
    "        # Path to input image\n",
    "        image_path = os.path.join(input_folder, image_file)\n",
    "\n",
    "        # Detect faces using RetinaFace\n",
    "        resp = RetinaFace.detect_faces(image_path)\n",
    "\n",
    "        # Load the image\n",
    "        img = cv2.imread(image_path)\n",
    "\n",
    "        # Iterate over detected faces\n",
    "        for face_id, face_data in resp.items():\n",
    "            # Extract facial area coordinates\n",
    "            x1, y1, x2, y2 = face_data['facial_area']\n",
    "            \n",
    "            # Extract the face region\n",
    "            face_img = img[y1:y2, x1:x2]\n",
    "\n",
    "            # Convert the face image to PIL image format\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "            # Draw the bounding box on the image\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "\n",
    "        # Save the processed image to the output folder\n",
    "        output_path = os.path.join(output_folder, f\"processed_{image_file}\")\n",
    "        cv2.imwrite(output_path, img)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        process_time = end_time - start_time\n",
    "        print(f\"Processing time for {image_file}: {process_time:.2f} seconds\")\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "# Specify the input and output folders\n",
    "input_folder = \"input_folder\"\n",
    "output_folder = \"output_frames\"\n",
    "\n",
    "# Detect image and save processed images\n",
    "detect_image(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n",
      "Processing:   1%|          | 8/1145 [07:10<16:45:32, 53.06s/it]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "from retinaface import RetinaFace\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_video(input_video_path, output_video_path):\n",
    "    # Load the emotion detection pipeline\n",
    "    emotion_pipe = pipeline('image-classification', model='Saravanan290702/facial_emotions_image_detection')\n",
    "\n",
    "    # Create a VideoCapture object\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open the video file.\")\n",
    "        return\n",
    "\n",
    "    # Get the video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize tqdm to show progress bar\n",
    "    progress_bar = tqdm(total=frame_count, desc=\"Processing\")\n",
    "\n",
    "    # Process each frame of the video\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect faces using RetinaFace\n",
    "        resp = RetinaFace.detect_faces(frame)\n",
    "\n",
    "        # Iterate over detected faces\n",
    "        for face_id, face_data in resp.items():\n",
    "            # Extract facial area coordinates\n",
    "            x1, y1, x2, y2 = face_data['facial_area']\n",
    "            \n",
    "            # Extract the face region\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Convert the face image to PIL image format\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "            \n",
    "            # Draw the bounding box on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Release VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(\"Processing complete. Output video saved successfully.\")\n",
    "\n",
    "# Specify the input and output video paths\n",
    "input_video_path = \"1.mp4\"\n",
    "output_video_path = \"output_video.avi\"\n",
    "\n",
    "# Detect emotions and draw bounding boxes on the input video frames, and output the processed video\n",
    "detect_video(input_video_path, output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from retinaface import RetinaFace\n",
    "from transformers import pipeline\n",
    "\n",
    "def detect_webcam():\n",
    "    \n",
    "    # Create a VideoCapture object for webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open the webcam.\")\n",
    "        return\n",
    "\n",
    "    # Get the webcam properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = 15  # Assuming a standard webcam captures at 30 frames per second\n",
    "\n",
    "    # Create a VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('output_video.avi', fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Process each frame from the webcam\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Detect faces using RetinaFace\n",
    "        resp = RetinaFace.detect_faces(frame)\n",
    "\n",
    "        # Iterate over detected faces\n",
    "        for face_id, face_data in resp.items():\n",
    "            # Extract facial area coordinates\n",
    "            x1, y1, x2, y2 = face_data['facial_area']\n",
    "\n",
    "            # Extract the face region\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Convert the face image to PIL image format\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Draw the bounding box on the frame\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Webcam Detection', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release VideoCapture and VideoWriter objects\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(\"Processing complete. Output video saved successfully.\")\n",
    "\n",
    "# Call the function to start webcam detection\n",
    "detect_webcam()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 49/49 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "from retinaface import RetinaFace\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_image(image_path):\n",
    "    \n",
    "    # Detect faces using RetinaFace\n",
    "    resp = RetinaFace.detect_faces(image_path)\n",
    "\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Initialize tqdm to show progress bar\n",
    "    progress_bar = tqdm(total=len(resp), desc=\"Processing\")\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for face_id, face_data in resp.items():\n",
    "        # Extract facial area coordinates\n",
    "        x1, y1, x2, y2 = face_data['facial_area']\n",
    "        \n",
    "        # Extract the face region\n",
    "        face_img = img[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert the face image to PIL image format\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Draw the bounding box on the image\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    return img\n",
    "\n",
    "# Specify the path to the input image\n",
    "image_path = \"masterimage.jpg\"\n",
    "\n",
    "# Detect emotions and draw bounding boxes on the image\n",
    "result_image = detect_image(image_path)\n",
    "\n",
    "# Save the image with the bounding boxes and predicted emotions drawn to disk\n",
    "cv2.imwrite('5-out.jpg', result_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
